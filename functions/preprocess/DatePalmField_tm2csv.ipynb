{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f8550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- fast, memory-safe NDVI → per-polygon median time series ---\n",
    "import os, re, glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from shapely.geometry import mapping, box\n",
    "from shapely.validation import make_valid\n",
    "from dask.distributed import Client\n",
    "from dask import delayed, compute\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# ----------------------------\n",
    "# Paths\n",
    "# ----------------------------\n",
    "top_dir = \"/datawaha/esom/DatePalmCounting\"\n",
    "satellite_dir = f\"{top_dir}/SatelliteData/Sentinel_2_NDVI_biweek_mean\"\n",
    "poly_path = f\"{top_dir}/Geoportal/Datepalm/app_server/datepalms/Qassim_datepalm_fields_polygons.geojson\"\n",
    "OUT_DIR = Path(top_dir) / \"Geoportal/Datepalm/app_server\" / \"ndvi_timeseries_csvs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# All years/tiles (adjust pattern if needed)\n",
    "satellite_list = sorted(glob.glob(f\"{satellite_dir}/*38RLQ-0000000000-0000000000.tif\"))\n",
    "assert satellite_list, \"No GeoTIFFs found.\"\n",
    "\n",
    "# ----------------------------\n",
    "# Dask client (bounded workers; spills when needed)\n",
    "# ----------------------------\n",
    "N_WORKERS = min(8, max(2, (os.cpu_count() or 4) - 1))\n",
    "client = Client(n_workers=N_WORKERS, threads_per_worker=1, processes=True)\n",
    "print(\"Dask dashboard:\", client.dashboard_link)\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers (no global compute)\n",
    "# ----------------------------\n",
    "def band_to_date(year: int, band_index: int) -> np.datetime64:\n",
    "    m = (band_index - 1) // 2 + 1\n",
    "    d = 1 if band_index % 2 == 1 else 16\n",
    "    return np.datetime64(f\"{year:04d}-{m:02d}-{d:02d}\")\n",
    "\n",
    "def year_from_name(path: str | Path) -> int:\n",
    "    s = Path(path).name\n",
    "    m = re.search(r\"(20(1[0-9]|2[0-9]))\", s)  # 2010–2029\n",
    "    if not m:\n",
    "        raise ValueError(f\"Cannot parse year from filename: {s}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def maybe_scale_ndvi_lazy(da: xr.DataArray, assume_scale=10000) -> xr.DataArray:\n",
    "    \"\"\"Avoid global min/max. If integer dtype, assume NDVI is scaled by 1e4.\"\"\"\n",
    "    if np.issubdtype(da.dtype, np.integer) and da.dtype != bool:\n",
    "        da = da.astype(\"float32\") / float(assume_scale)\n",
    "    else:\n",
    "        da = da.astype(\"float32\")\n",
    "    nd = da.rio.nodata\n",
    "    if nd is not None:\n",
    "        da = da.where(da != nd)\n",
    "    # keep reasonable range\n",
    "    return da.where((da >= -1.2) & (da <= 1.2))\n",
    "\n",
    "def interp_extend(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "    s = (df.set_index(\"date\")[\"ndvi_median\"]\n",
    "          .replace(0, np.nan)\n",
    "          .interpolate(method=\"time\", limit_direction=\"both\")\n",
    "          .ffill().bfill().clip(-1.0, 1.0))\n",
    "    df[\"ndvi_median\"] = s.values\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Build LAZY time-cube (no compute, no persist)\n",
    "# ----------------------------\n",
    "CHUNKS = {\"band\": 1, \"y\": 1024, \"x\": 1024}   # smaller chunks -> lower peak RAM\n",
    "\n",
    "da_list = []\n",
    "for tif in satellite_list:\n",
    "    year = year_from_name(tif)\n",
    "    da = rxr.open_rasterio(tif, chunks=CHUNKS)  # (band,y,x) dask-backed\n",
    "\n",
    "    # Rename band→time & assign dates (no conflict)\n",
    "    dates = [band_to_date(year, b) for b in range(1, int(da.sizes[\"band\"]) + 1)]\n",
    "    da = da.rename({\"band\": \"time\"})\n",
    "    da = da.assign_coords(time=(\"time\", np.array(dates, dtype=\"datetime64[D]\")))\n",
    "    da.name = \"ndvi\"\n",
    "    da_list.append(da)\n",
    "\n",
    "# Concatenate lazily\n",
    "NDVI = xr.concat(da_list, dim=\"time\").sortby(\"time\")\n",
    "# Scale lazily (no .min/.max)\n",
    "NDVI = maybe_scale_ndvi_lazy(NDVI)\n",
    "# DO NOT .compute() or .persist() here – keep it lazy to avoid huge RAM use.\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Load polygons (match CRS exactly)\n",
    "# ----------------------------\n",
    "polys = gpd.read_file(poly_path)\n",
    "ID_COL = \"Field_id\" if \"Field_id\" in polys.columns else (\n",
    "         \"field_id\" if \"field_id\" in polys.columns else (\n",
    "         \"poly_id\"  if \"poly_id\"  in polys.columns else None))\n",
    "if ID_COL is None:\n",
    "    polys = polys.reset_index(names=\"field_id\"); ID_COL = \"field_id\"\n",
    "\n",
    "# Ensure CRS matches cube (your earlier code forced EPSG:32638; we’ll align to raster instead)\n",
    "assert NDVI.rio.crs is not None, \"Raster has no CRS.\"\n",
    "if polys.crs != NDVI.rio.crs:\n",
    "    polys = polys.to_crs(NDVI.rio.crs)\n",
    "\n",
    "# Fix invalid geometries once\n",
    "polys[\"geometry\"] = polys.geometry.map(make_valid)\n",
    "\n",
    "# Optional: subset for testing\n",
    "# polys = polys.iloc[0:500]\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Define per-polygon worker (bbox→clip→median→interpolate→CSV)\n",
    "# ----------------------------\n",
    "@delayed\n",
    "def process_one_polygon(row, out_dir=OUT_DIR, all_touched=True, pad_px=1):\n",
    "    pid = int(row[ID_COL])\n",
    "    out_csv = out_dir / f\"{pid}.csv\"\n",
    "    if out_csv.exists():\n",
    "        return pid, \"skip\"\n",
    "\n",
    "    geom = row.geometry\n",
    "    # quick bounding-box reject against global raster bounds\n",
    "    rb = box(*NDVI.rio.bounds())\n",
    "    if not rb.intersects(geom):\n",
    "        out_csv.write_text(\"date,ndvi_median\\n\")\n",
    "        return pid, \"no_coverage\"\n",
    "\n",
    "    # First trim by bbox (much cheaper), then precise clip\n",
    "    # Use a small padding in CRS units based on pixel size to avoid empty trims\n",
    "    dx = abs(NDVI.rio.transform()[0])\n",
    "    dy = abs(NDVI.rio.transform()[4])\n",
    "    minx, miny, maxx, maxy = geom.bounds\n",
    "    sub = NDVI.rio.clip_box(minx - pad_px*dx, miny - pad_px*dy,\n",
    "                            maxx + pad_px*dx, maxy + pad_px*dy)\n",
    "    sub = sub.rio.clip([mapping(geom)], all_touched=all_touched, drop=True)\n",
    "\n",
    "    if sub.size == 0 or sub.isnull().all():\n",
    "        out_csv.write_text(\"date,ndvi_median\\n\")\n",
    "        return pid, \"empty\"\n",
    "\n",
    "    # median over (y,x) per time -> Dask reduction, tiny output\n",
    "    med_ts = sub.median(dim=(\"y\", \"x\"), skipna=True)\n",
    "\n",
    "    # Materialize tiny vector only\n",
    "    dates = pd.to_datetime(med_ts[\"time\"].values.astype(\"datetime64[D]\"))\n",
    "    vals  = med_ts.compute().values  # <= len(time) numbers\n",
    "\n",
    "    df = pd.DataFrame({\"date\": dates, \"ndvi_median\": vals})\n",
    "    df = interp_extend(df)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    return pid, \"ok\"\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Dispatch with bounded parallelism\n",
    "# ----------------------------\n",
    "tasks = [process_one_polygon(row) for _, row in polys.iterrows()]\n",
    "with ProgressBar():\n",
    "    results = compute(*tasks)\n",
    "\n",
    "ok = sum(1 for _, s in results if s == \"ok\")\n",
    "print(f\"✓ wrote {ok} CSVs → {OUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datepalmprj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
